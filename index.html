<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Qingqing Cao</title> <meta name="author" content="Qingqing Cao"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="qingqing cao, research scientist, nlp, ai, efficiency, efficient ai, efficient nlp, llm, efficient llm"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://awk.ai/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Qingqing</span> Cao </h1> <p class="desc">Research Scientist ‚Ä¢ Ô£ø Apple AIML.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg?265980fef2fdcbf3c187e7ccc5b5d335" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am a research scientist at Apple AIML. My research interests include efficient NLP, mobile computing, and ML systems. I have focused on building efficient and practical NLP systems for both edge devices and the cloud, such as on-device (visual) question answering and faster Transformer models.</p> <p>Previously, I was a postdoc in the <a href="https://www.cs.washington.edu/research/nlp" rel="external nofollow noopener" target="_blank">UW NLP group</a> at the University of Washington where I won the <a href="https://www.cs.washington.edu/academics/postdoc/research-awards/recipients" rel="external nofollow noopener" target="_blank">postdoc research award</a> twice. I hold a Ph.D. degree in computer science at <a href="https://www.cs.stonybrook.edu/" rel="external nofollow noopener" target="_blank">Stony Brook University</a>. I was a recipient of the <a href="https://www.cs.stonybrook.edu/about-us/News/Funding-Doctoral-Research-Catacosinos-Fellowship-Awardees-2021" rel="external nofollow noopener" target="_blank">Catacosinos Fellowship</a> at Stony Brook University and a <a href="https://datascience.uchicago.edu/research/postdoctoral-programs/rising-stars/2021/" rel="external nofollow noopener" target="_blank">Rising Star in Data Science</a> at the University of Chicago.</p> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%69%6D@%61%77%6B.%61%69" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=vLpPyUUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/31961604" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/csarron" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/qqcao" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/awk_ai" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> Please use <a href="mailto:qicao@apple.com">qicao@apple.com</a> for Apple work, otherwise use <a href="mailto:im@awk.ai">im@awk.ai</a> or <a href="/assets/img/wx.jpg">wechat</a>. </div> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%;font-family: monospace;">Nov 02, 2024</th> <td> Thanks NeurIPS for recognizing me as a <a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers" style="color: orangered;" rel="external nofollow noopener" target="_blank">top reviewer (8.6%)</a>! </td> </tr> <tr> <th scope="row" style="width: 20%;font-family: monospace;">Oct 23, 2024</th> <td> Check out my new paper on controllable data synthesis: <a href="https://machinelearning.apple.com/research/controlled-synthesis" rel="external nofollow noopener" target="_blank">CtrlSynth</a>. </td> </tr> <tr> <th scope="row" style="width: 20%;font-family: monospace;">Oct 21, 2024</th> <td> Check out our new papers on making LLMs more efficient: <a href="https://arxiv.org/abs/2410.08391" rel="external nofollow noopener" target="_blank">KV Prediction</a> and <a href="https://arxiv.org/abs/2410.14072" rel="external nofollow noopener" target="_blank">Victor</a>. </td> </tr> <tr> <th scope="row" style="width: 20%;font-family: monospace;">Jun 10, 2024</th> <td> <a href="https://developer.apple.com/videos/play/wwdc2024/10223/?time=965" rel="external nofollow noopener" target="_blank">Apple WWDC 2024</a> highlighted OpenELM! It is also on <a href="https://machinelearning.apple.com/research/openelm" rel="external nofollow noopener" target="_blank">Apple Machine Learning Research</a>. </td> </tr> <tr> <th scope="row" style="width: 20%;font-family: monospace;">May 01, 2024</th> <td> <a href="/publications/#zhaoAPTAdaptivePruning2024">APT</a> got accepted to ICML 2024 as an <a href="https://icml.cc/virtual/2024/events/oral#event-35453" style="color: orangered;" rel="external nofollow noopener" target="_blank">oral (1.5%)</a> paper üéâ! Congrats to <a href="https://roim1998.github.io/" rel="external nofollow noopener" target="_blank">Bowen</a> üëè! </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">Recent publications</a></h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arxiv</abbr></div> <div id="caoCtrlSynthControllableImage2024" class="col-sm-8"> <div class="title"> <a href="#caoCtrlSynthControllableImage2024">#</a> CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>,¬†<a href="https://www.mahyarnajibi.com/" rel="external nofollow noopener" target="_blank">Mahyar Najibi</a>,¬†and¬†<a href="https://sacmehta.github.io/" rel="external nofollow noopener" target="_blank">Sachin Mehta</a> </div> <div class="periodical"> Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2410.11963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="http://arxiv.org/abs/2410.11963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Pretraining robust vision or multimodal foundation models (e.g., CLIP) relies on large-scale datasets that may be noisy, potentially misaligned, and have long-tail distributions. Previous works have shown promising results in augmenting datasets by generating synthetic samples. However, they only support domain-specific ad hoc use cases (e.g., either image or text only, but not both), and are limited in data diversity due to a lack of fine-grained control over the synthesis process. In this paper, we design a }emph{controllable} image-text synthesis pipeline, CtrlSynth, for data-efficient and robust multimodal learning. The key idea is to decompose the visual semantics of an image into basic elements, apply user-specified control policies (e.g., remove, add, or replace operations), and recompose them to synthesize images or texts. The decompose and recompose feature in CtrlSynth allows users to control data synthesis in a fine-grained manner by defining customized control policies to manipulate the basic elements. CtrlSynth leverages the capabilities of pretrained foundation models such as large language models or diffusion models to reason and recompose basic elements such that synthetic samples are natural and composed in diverse ways. CtrlSynth is a closed-loop, training-free, and modular framework, making it easy to support different pretrained models. With extensive experiments on 31 datasets spanning different vision and vision-language tasks, we show that CtrlSynth substantially improves zero-shot classification, image-text retrieval, and compositional reasoning performance of CLIP models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">caoCtrlSynthControllableImage2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{CtrlSynth}: {Controllable} {Image} {Text} {Synthesis} for {Data}-{Efficient} {Multimodal} {Learning}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{CtrlSynth}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2410.11963}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-10-23}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Najibi, Mahyar and Mehta, Sachin}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arxiv</abbr></div> <div id="hortonKVPredictionImproved2024" class="col-sm-8"> <div class="title"> <a href="#hortonKVPredictionImproved2024">#</a> KV Prediction for Improved Time to First Token</div> <div class="author"> <a href="https://mchorton.com/" rel="external nofollow noopener" target="_blank">Maxwell Horton</a>,¬†<em><strong>Qingqing Cao</strong></em>,¬†<a href="https://www.linkedin.com/in/chenfansun" rel="external nofollow noopener" target="_blank">Chenfan Sun</a>,¬†<a href="https://www.linkedin.com/in/yanzi-jin-112a3137" rel="external nofollow noopener" target="_blank">Yanzi Jin</a>,¬†<a href="https://sacmehta.github.io/" rel="external nofollow noopener" target="_blank">Sachin Mehta</a>,¬†<a href="https://mrastegari.github.io/" rel="external nofollow noopener" target="_blank">Mohammad Rastegari</a>,¬†and¬†Moin Nabi</div> <div class="periodical"> Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2410.08391" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="http://arxiv.org/abs/2410.08391" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model‚Äôs outputs. To reduce the time spent producing the first output (known as the ‚Äútime to first token‚Äù, or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of \15}%-50}% across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to \30}% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction .</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">hortonKVPredictionImproved2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{KV} {Prediction} for {Improved} {Time} to {First} {Token}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2410.08391}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-10-24}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horton, Maxwell and Cao, Qingqing and Sun, Chenfan and Jin, Yanzi and Mehta, Sachin and Rastegari, Mohammad and Nabi, Moin}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arxiv</abbr></div> <div id="wenEfficientVisionLanguageModels2024" class="col-sm-8"> <div class="title"> <a href="#wenEfficientVisionLanguageModels2024">#</a> Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers</div> <div class="author"> Yuxin Wen,¬†<em><strong>Qingqing Cao</strong></em>,¬†Qichen Fu,¬†<a href="https://sacmehta.github.io/" rel="external nofollow noopener" target="_blank">Sachin Mehta</a>,¬†and¬†<a href="https://www.mahyarnajibi.com/" rel="external nofollow noopener" target="_blank">Mahyar Najibi</a> </div> <div class="periodical"> Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2410.14072" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="http://arxiv.org/abs/2410.14072" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Recent advancements in vision-language models (VLMs) have expanded their potential for real-world applications, enabling these models to perform complex reasoning on images. In the widely used fully autoregressive transformer-based models like LLaVA, projected visual tokens are prepended to textual tokens. Oftentimes, visual tokens are significantly more than prompt tokens, resulting in increased computational overhead during both training and inference. In this paper, we propose Visual Compact Token Registers (Victor), a method that reduces the number of visual tokens by summarizing them into a smaller set of register tokens. Victor adds a few learnable register tokens after the visual tokens and summarizes the visual information into these registers using the first few layers in the language tower of VLMs. After these few layers, all visual tokens are discarded, significantly improving computational efficiency for both training and inference. Notably, our method is easy to implement and requires a small number of new trainable parameters with minimal impact on model performance. In our experiment, with merely 8 visual registers‚Äìabout 1% of the original tokens‚ÄìVictor shows less than a 4% accuracy drop while reducing the total training time by 43% and boosting the inference throughput by 3.3X.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">wenEfficientVisionLanguageModels2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient {Vision}-{Language} {Models} by {Summarizing} {Visual} {Tokens} into {Compact} {Registers}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2410.14072}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-10-24}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wen, Yuxin and Cao, Qingqing and Fu, Qichen and Mehta, Sachin and Najibi, Mahyar}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://es-fomo.com/" rel="external nofollow noopener" target="_blank">ES-FoMo 2024</a></abbr></div> <div id="mehtaOpenELMEfficientLanguage2024" class="col-sm-8"> <div class="title"> <a href="#mehtaOpenELMEfficientLanguage2024">#</a> OpenELM: An Efficient Language Model Family with Open Training and Inference Framework</div> <div class="author"> <a href="https://sacmehta.github.io/" rel="external nofollow noopener" target="_blank">Sachin Mehta</a>,¬†<a href="https://ca.linkedin.com/in/mhsekhavat" rel="external nofollow noopener" target="_blank">Mohammad Hossein Sekhavat</a>,¬†<em><strong>Qingqing Cao</strong></em>,¬†<a href="https://mchorton.com/" rel="external nofollow noopener" target="_blank">Maxwell Horton</a>,¬†<a href="https://www.linkedin.com/in/yanzi-jin-112a3137" rel="external nofollow noopener" target="_blank">Yanzi Jin</a>,¬†<a href="https://www.linkedin.com/in/chenfansun" rel="external nofollow noopener" target="_blank">Chenfan Sun</a>,¬†<a href="https://imirzadeh.me/" rel="external nofollow noopener" target="_blank">Iman Mirzadeh</a>,¬†<a href="https://www.mahyarnajibi.com/" rel="external nofollow noopener" target="_blank">Mahyar Najibi</a>,¬†<a href="https://www.linkedin.com/in/dmitrybelenko" rel="external nofollow noopener" target="_blank">Dmitry Belenko</a>,¬†<a href="https://www.linkedin.com/in/peterzat" rel="external nofollow noopener" target="_blank">Peter Zatloukal</a>,¬†and¬†<a href="https://mrastegari.github.io/" rel="external nofollow noopener" target="_blank">Mohammad Rastegari</a> </div> <div class="periodical"> Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2404.14619" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2x fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. Our source code along with pre-trained model weights and training recipes is available at https://github.com/apple/corenet. Additionally, OpenELM models can be found on HuggingFace at: https://huggingface.co/apple/OpenELM.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">mehtaOpenELMEfficientLanguage2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{OpenELM}}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-04-29}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Workshop on Efficient Systems for Foundation Models II @ ICML2024}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://icml.cc/Conferences/2024" rel="external nofollow noopener" target="_blank">ICML 2024</a></abbr></div> <div id="zhaoAPTAdaptivePruning2024" class="col-sm-8"> <div class="title"> <a href="#zhaoAPTAdaptivePruning2024">#</a> APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference</div> <div class="author"> <a href="https://roim1998.github.io/" rel="external nofollow noopener" target="_blank">Bowen Zhao</a>,¬†<a href="https://homes.cs.washington.edu/~hannaneh/index.html" rel="external nofollow noopener" target="_blank">Hannaneh Hajishirzi</a>,¬†and¬†<em><strong>Qingqing Cao</strong></em> </div> <div class="periodical"> Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2401.12200" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://openreview.net/forum?id=sb81Xl50JG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://icml.cc/virtual/2024/events/oral#event-35453" class="btn btn-sm z-depth-0" style="font-weight: bold; color: orangered;" rel="external nofollow noopener" target="_blank">Oral (1.5%)</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models‚Äô performance with 70% parameters remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhaoAPTAdaptivePruning2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{APT}}: {{Adaptive Pruning}} and {{Tuning Pretrained Language Models}} for {{Efficient Training}} and {{Inference}}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{{APT}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Bowen and Hajishirzi, Hannaneh and Cao, Qingqing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=sb81Xl50JG}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{arXiv:2401.12200}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2401.12200}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{arXiv}}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-01-23}</span><span class="p">,</span>
  <span class="na">highlight</span> <span class="p">=</span> <span class="s">{Oral (1.5%)}</span><span class="p">,</span>
  <span class="na">highlight_url</span> <span class="p">=</span> <span class="s">{https://icml.cc/virtual/2024/events/oral#event-35453}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://iclr.cc/Conferences/2024" rel="external nofollow noopener" target="_blank">ICLR 2024</a></abbr></div> <div id="caoBTRBinaryToken2024" class="col-sm-8"> <div class="title"> <a href="#caoBTRBinaryToken2024">#</a> BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>,¬†<a href="https://shmsw25.github.io/" rel="external nofollow noopener" target="_blank">Sewon Min</a>,¬†<a href="https://homes.cs.washington.edu/~yizhongw/" rel="external nofollow noopener" target="_blank">Yizhong Wang</a>,¬†and¬†<a href="https://homes.cs.washington.edu/~hannaneh/index.html" rel="external nofollow noopener" target="_blank">Hannaneh Hajishirzi</a> </div> <div class="periodical"> <em>In </em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2310.01329" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://openreview.net/forum?id=3TO3TtnOFl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/BTR-poster-ICLR2024.pptx" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/btr-slides.pptx" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://iclr.cc/virtual/2024/poster/19511" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://iclr.cc/virtual/2024/poster/19511" class="btn btn-sm z-depth-0" style="font-weight: bold; color: orangered;" rel="external nofollow noopener" target="_blank">Spotlight (5%)</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance. Our code is publicly available at https://github.com/csarron/BTR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">caoBTRBinaryToken2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{BTR}: {Binary} {Token} {Representations} for {Efficient} {Retrieval} {Augmented} {Language} {Models}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{BTR}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=3TO3TtnOFl}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-04-29}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Min, Sewon and Wang, Yizhong and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">highlight</span> <span class="p">=</span> <span class="s">{Spotlight (5%)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container" align="center"> ¬© Copyright 2025 Qingqing Cao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="external nofollow noopener">al-folio</a> theme. Last updated: February, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>