<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Qingqing Cao</title> <meta name="author" content="Qingqing Cao"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar. *equal contribution."> <meta name="keywords" content="qingqing cao, research scientist, nlp, ai, efficiency, efficient ai, efficient nlp, llm, efficient llm"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://awk.ai/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Qingqing </span>Cao</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar. *equal contribution.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arxiv</abbr></div> <div id="caoCtrlSynthControllableImage2024" class="col-sm-8"> <div class="title"> <a href="#caoCtrlSynthControllableImage2024">#</a> CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="https://www.mahyarnajibi.com/" rel="external nofollow noopener" target="_blank">Mahyar Najibi</a>, and <a href="https://sacmehta.github.io/" rel="external nofollow noopener" target="_blank">Sachin Mehta</a> </div> <div class="periodical"> Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2410.11963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="http://arxiv.org/abs/2410.11963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Pretraining robust vision or multimodal foundation models (e.g., CLIP) relies on large-scale datasets that may be noisy, potentially misaligned, and have long-tail distributions. Previous works have shown promising results in augmenting datasets by generating synthetic samples. However, they only support domain-specific ad hoc use cases (e.g., either image or text only, but not both), and are limited in data diversity due to a lack of fine-grained control over the synthesis process. In this paper, we design a }emph{controllable} image-text synthesis pipeline, CtrlSynth, for data-efficient and robust multimodal learning. The key idea is to decompose the visual semantics of an image into basic elements, apply user-specified control policies (e.g., remove, add, or replace operations), and recompose them to synthesize images or texts. The decompose and recompose feature in CtrlSynth allows users to control data synthesis in a fine-grained manner by defining customized control policies to manipulate the basic elements. CtrlSynth leverages the capabilities of pretrained foundation models such as large language models or diffusion models to reason and recompose basic elements such that synthetic samples are natural and composed in diverse ways. CtrlSynth is a closed-loop, training-free, and modular framework, making it easy to support different pretrained models. With extensive experiments on 31 datasets spanning different vision and vision-language tasks, we show that CtrlSynth substantially improves zero-shot classification, image-text retrieval, and compositional reasoning performance of CLIP models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">caoCtrlSynthControllableImage2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{CtrlSynth}: {Controllable} {Image} {Text} {Synthesis} for {Data}-{Efficient} {Multimodal} {Learning}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{CtrlSynth}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2410.11963}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-10-23}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Najibi, Mahyar and Mehta, Sachin}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arxiv</abbr></div> <div id="hortonKVPredictionImproved2024" class="col-sm-8"> <div class="title"> <a href="#hortonKVPredictionImproved2024">#</a> KV Prediction for Improved Time to First Token</div> <div class="author"> <a href="https://mchorton.com/" rel="external nofollow noopener" target="_blank">Maxwell Horton</a>, <em><strong>Qingqing Cao</strong></em>, <a href="https://www.linkedin.com/in/chenfansun" rel="external nofollow noopener" target="_blank">Chenfan Sun</a>, <a href="https://www.linkedin.com/in/yanzi-jin-112a3137" rel="external nofollow noopener" target="_blank">Yanzi Jin</a>, <a href="https://sacmehta.github.io/" rel="external nofollow noopener" target="_blank">Sachin Mehta</a>, <a href="https://mrastegari.github.io/" rel="external nofollow noopener" target="_blank">Mohammad Rastegari</a>, and Moin Nabi</div> <div class="periodical"> Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2410.08391" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="http://arxiv.org/abs/2410.08391" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model’s outputs. To reduce the time spent producing the first output (known as the “time to first token”, or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of \15}%-50}% across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to \30}% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction .</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">hortonKVPredictionImproved2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{KV} {Prediction} for {Improved} {Time} to {First} {Token}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2410.08391}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-10-24}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horton, Maxwell and Cao, Qingqing and Sun, Chenfan and Jin, Yanzi and Mehta, Sachin and Rastegari, Mohammad and Nabi, Moin}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arxiv</abbr></div> <div id="wenEfficientVisionLanguageModels2024" class="col-sm-8"> <div class="title"> <a href="#wenEfficientVisionLanguageModels2024">#</a> Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers</div> <div class="author"> Yuxin Wen, <em><strong>Qingqing Cao</strong></em>, Qichen Fu, <a href="https://sacmehta.github.io/" rel="external nofollow noopener" target="_blank">Sachin Mehta</a>, and <a href="https://www.mahyarnajibi.com/" rel="external nofollow noopener" target="_blank">Mahyar Najibi</a> </div> <div class="periodical"> Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2410.14072" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="http://arxiv.org/abs/2410.14072" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Recent advancements in vision-language models (VLMs) have expanded their potential for real-world applications, enabling these models to perform complex reasoning on images. In the widely used fully autoregressive transformer-based models like LLaVA, projected visual tokens are prepended to textual tokens. Oftentimes, visual tokens are significantly more than prompt tokens, resulting in increased computational overhead during both training and inference. In this paper, we propose Visual Compact Token Registers (Victor), a method that reduces the number of visual tokens by summarizing them into a smaller set of register tokens. Victor adds a few learnable register tokens after the visual tokens and summarizes the visual information into these registers using the first few layers in the language tower of VLMs. After these few layers, all visual tokens are discarded, significantly improving computational efficiency for both training and inference. Notably, our method is easy to implement and requires a small number of new trainable parameters with minimal impact on model performance. In our experiment, with merely 8 visual registers–about 1% of the original tokens–Victor shows less than a 4% accuracy drop while reducing the total training time by 43% and boosting the inference throughput by 3.3X.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">wenEfficientVisionLanguageModels2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient {Vision}-{Language} {Models} by {Summarizing} {Visual} {Tokens} into {Compact} {Registers}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2410.14072}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-10-24}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wen, Yuxin and Cao, Qingqing and Fu, Qichen and Mehta, Sachin and Najibi, Mahyar}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://es-fomo.com/" rel="external nofollow noopener" target="_blank">ES-FoMo 2024</a></abbr></div> <div id="mehtaOpenELMEfficientLanguage2024" class="col-sm-8"> <div class="title"> <a href="#mehtaOpenELMEfficientLanguage2024">#</a> OpenELM: An Efficient Language Model Family with Open Training and Inference Framework</div> <div class="author"> <a href="https://sacmehta.github.io/" rel="external nofollow noopener" target="_blank">Sachin Mehta</a>, <a href="https://ca.linkedin.com/in/mhsekhavat" rel="external nofollow noopener" target="_blank">Mohammad Hossein Sekhavat</a>, <em><strong>Qingqing Cao</strong></em>, <a href="https://mchorton.com/" rel="external nofollow noopener" target="_blank">Maxwell Horton</a>, <a href="https://www.linkedin.com/in/yanzi-jin-112a3137" rel="external nofollow noopener" target="_blank">Yanzi Jin</a>, <a href="https://www.linkedin.com/in/chenfansun" rel="external nofollow noopener" target="_blank">Chenfan Sun</a>, <a href="https://imirzadeh.me/" rel="external nofollow noopener" target="_blank">Iman Mirzadeh</a>, <a href="https://www.mahyarnajibi.com/" rel="external nofollow noopener" target="_blank">Mahyar Najibi</a>, <a href="https://www.linkedin.com/in/dmitrybelenko" rel="external nofollow noopener" target="_blank">Dmitry Belenko</a>, <a href="https://www.linkedin.com/in/peterzat" rel="external nofollow noopener" target="_blank">Peter Zatloukal</a>, and <a href="https://mrastegari.github.io/" rel="external nofollow noopener" target="_blank">Mohammad Rastegari</a> </div> <div class="periodical"> Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2404.14619" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2x fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. Our source code along with pre-trained model weights and training recipes is available at https://github.com/apple/corenet. Additionally, OpenELM models can be found on HuggingFace at: https://huggingface.co/apple/OpenELM.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">mehtaOpenELMEfficientLanguage2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{OpenELM}}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-04-29}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Workshop on Efficient Systems for Foundation Models II @ ICML2024}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://icml.cc/Conferences/2024" rel="external nofollow noopener" target="_blank">ICML 2024</a></abbr></div> <div id="zhaoAPTAdaptivePruning2024" class="col-sm-8"> <div class="title"> <a href="#zhaoAPTAdaptivePruning2024">#</a> APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference</div> <div class="author"> <a href="https://roim1998.github.io/" rel="external nofollow noopener" target="_blank">Bowen Zhao</a>, <a href="https://homes.cs.washington.edu/~hannaneh/index.html" rel="external nofollow noopener" target="_blank">Hannaneh Hajishirzi</a>, and <em><strong>Qingqing Cao</strong></em> </div> <div class="periodical"> Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2401.12200" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://openreview.net/forum?id=sb81Xl50JG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://icml.cc/virtual/2024/events/oral#event-35453" class="btn btn-sm z-depth-0" style="font-weight: bold; color: orangered;" rel="external nofollow noopener" target="_blank">Oral (1.5%)</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models’ performance with 70% parameters remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhaoAPTAdaptivePruning2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{APT}}: {{Adaptive Pruning}} and {{Tuning Pretrained Language Models}} for {{Efficient Training}} and {{Inference}}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{{APT}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Bowen and Hajishirzi, Hannaneh and Cao, Qingqing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=sb81Xl50JG}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{arXiv:2401.12200}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2401.12200}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{arXiv}}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-01-23}</span><span class="p">,</span>
  <span class="na">highlight</span> <span class="p">=</span> <span class="s">{Oral (1.5%)}</span><span class="p">,</span>
  <span class="na">highlight_url</span> <span class="p">=</span> <span class="s">{https://icml.cc/virtual/2024/events/oral#event-35453}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://iclr.cc/Conferences/2024" rel="external nofollow noopener" target="_blank">ICLR 2024</a></abbr></div> <div id="caoBTRBinaryToken2024" class="col-sm-8"> <div class="title"> <a href="#caoBTRBinaryToken2024">#</a> BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="https://shmsw25.github.io/" rel="external nofollow noopener" target="_blank">Sewon Min</a>, <a href="https://homes.cs.washington.edu/~yizhongw/" rel="external nofollow noopener" target="_blank">Yizhong Wang</a>, and <a href="https://homes.cs.washington.edu/~hannaneh/index.html" rel="external nofollow noopener" target="_blank">Hannaneh Hajishirzi</a> </div> <div class="periodical"> <em>In </em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2310.01329" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://openreview.net/forum?id=3TO3TtnOFl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/BTR-poster-ICLR2024.pptx" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/btr-slides.pptx" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://iclr.cc/virtual/2024/poster/19511" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://iclr.cc/virtual/2024/poster/19511" class="btn btn-sm z-depth-0" style="font-weight: bold; color: orangered;" rel="external nofollow noopener" target="_blank">Spotlight (5%)</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance. Our code is publicly available at https://github.com/csarron/BTR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">caoBTRBinaryToken2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{BTR}: {Binary} {Token} {Representations} for {Efficient} {Retrieval} {Augmented} {Language} {Models}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{BTR}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=3TO3TtnOFl}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-04-29}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Min, Sewon and Wang, Yizhong and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">highlight</span> <span class="p">=</span> <span class="s">{Spotlight (5%)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div> <div id="peng2023efficiency" class="col-sm-8"> <div class="title"> <a href="#peng2023efficiency">#</a> Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation</div> <div class="author"> <a href="https://haopeng-nlp.github.io/" rel="external nofollow noopener" target="_blank">Hao Peng</a>, <em><strong>Qingqing Cao</strong></em>, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, <a href="https://nasmith.github.io/" rel="external nofollow noopener" target="_blank">Noah A. Smith</a>, and <a href="https://homes.cs.washington.edu/~hannaneh/index.html" rel="external nofollow noopener" target="_blank">Hannaneh Hajishirzi</a> </div> <div class="periodical"> Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2307.09701" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model’s lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">peng2023efficiency</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peng, Hao and Cao, Qingqing and Dodge, Jesse and Peters, Matthew E. and Fernandez, Jared and Sherborne, Tom and Lo, Kyle and Skjonsberg, Sam and Strubell, Emma and Plessas, Darrell and Beltagy, Iz and Walsh, Evan Pete and Smith, Noah A. and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://nips.cc/Conferences/2023" rel="external nofollow noopener" target="_blank">NeurIPS 2023</a></abbr></div> <div id="regeAdANNSFrameworkAdaptive2023" class="col-sm-8"> <div class="title"> <a href="#regeAdANNSFrameworkAdaptive2023">#</a> AdANNS: A Framework for Adaptive Semantic Search</div> <div class="author"> <a href="https://aniketrege.github.io/" rel="external nofollow noopener" target="_blank">Aniket Rege</a>, <a href="https://homes.cs.washington.edu/~kusupati/" rel="external nofollow noopener" target="_blank">Aditya Kusupati</a>, Sharan Ranjit S, Alan Fan, <em><strong>Qingqing Cao</strong></em>, Sham M. Kakade, Prateek Jain, and Ali Farhadi</div> <div class="periodical"> <em>In Thirty-Seventh Conference on Neural Information Processing Systems</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2305.19435" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are _rigid, high-dimensional_ vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage _adaptive representations_ of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is up to {}mathbf{1.5}\% more accurate than the rigid representations-based IVF at the same compute budget; and matches accuracy while being up to {}mathbf{90}}times faster in _wall-clock time_. For Natural Questions, \32\-byte AdANNS-OPQ matches the accuracy of the \64\-byte OPQ baseline constructed using rigid representations – _same accuracy at half the cost!_ We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that AdANNS can enable inference-time adaptivity for compute-aware search on ANNS indices built non-adaptively on matryoshka representations. Code is open-sourced at https://github.com/RAIVNLab/AdANNS.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">regeAdANNSFrameworkAdaptive2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{AdANNS}}: {{A Framework}} for {{Adaptive Semantic Search}}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{{AdANNS}}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rege, Aniket and Kusupati, Aditya and S, Sharan Ranjit and Fan, Alan and Cao, Qingqing and Kakade, Sham M. and Jain, Prateek and Farhadi, Ali}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-01-24}</span><span class="p">,</span>
  <span class="na">langid</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://2023.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL 2023</a></abbr></div> <div id="cao-etal-2023-pumer" class="col-sm-8"> <div class="title"> <a href="#cao-etal-2023-pumer">#</a> PuMer: Pruning and Merging Tokens for Efficient Vision Language Models</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="https://bhargaviparanjape.github.io/" rel="external nofollow noopener" target="_blank">Bhargavi Paranjape</a>, and <a href="https://homes.cs.washington.edu/~hannaneh/index.html" rel="external nofollow noopener" target="_blank">Hannaneh Hajishirzi</a> </div> <div class="periodical"> <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2305.17530" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://aclanthology.org/2023.acl-long.721" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/csarron/PuMer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/pumer-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/pumer-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Large-scale vision language (VL) models use Transformers to perform cross-modal interactions between the input text and image. These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text. We present PuMer: a token reduction framework that uses text-informed Pruning and modality-aware Merging strategies to progressively reduce the tokens of input image and text, improving model inference speed and reducing memory footprint. PuMer learns to keep salient image tokens related to the input text and merges similar textual and visual tokens by adding lightweight token reducer modules at several cross-modal layers in the VL model. Training PuMer is mostly the same as finetuning the original VL model but faster. Our evaluation for two vision language models on four downstream VL tasks shows PuMer increases inference throughput by up to 2x and reduces memory footprint by over 50% while incurring less than a 1% accuracy drop.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cao-etal-2023-pumer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{P}u{M}er: Pruning and Merging Tokens for Efficient Vision Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Paranjape, Bhargavi and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.721}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.acl-long.721}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12890--12903}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://2023.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL 2023</a></abbr></div> <div id="zhang-etal-2023-survey-efficient" class="col-sm-8"> <div class="title"> <a href="#zhang-etal-2023-survey-efficient">#</a> A Survey for Efficient Open Domain Question Answering</div> <div class="author"> Qin Zhang, Shangsi Chen, Dongkuan Xu, <em><strong>Qingqing Cao</strong></em>, Xiaojun Chen, Trevor Cohn, and Meng Fang</div> <div class="periodical"> <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2211.07886" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://aclanthology.org/2023.acl-long.808" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Open domain question answering (ODQA) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing (NLP). Recent works have predominantly focused on improving the answering accuracy and have achieved promising progress. However, higher accuracy often requires more memory consumption and inference latency, which might not necessarily be efficient enough for direct deployment in the real world. Thus, a trade-off between accuracy, memory consumption and processing speed is pursued. In this paper, we will survey recent advancements in the efficiency of ODQA models and conclude core techniques for achieving efficiency. Additionally, we will provide a quantitative analysis of memory cost, query speed, accuracy, and overall performance comparison. Our goal is to keep scholars informed of the latest advancements and open challenges in ODQA efficiency research and contribute to the further development of ODQA efficiency.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang-etal-2023-survey-efficient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Survey for Efficient Open Domain Question Answering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Qin and Chen, Shangsi and Xu, Dongkuan and Cao, Qingqing and Chen, Xiaojun and Cohn, Trevor and Fang, Meng}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.808}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.acl-long.808}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{14447--14465}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://transacl.org/index.php/tacl/issue/view/27" rel="external nofollow noopener" target="_blank">TACL 2023</a></abbr></div> <div id="trevisoEfficientMethodsNatural2023" class="col-sm-8"> <div class="title"> <a href="#trevisoEfficientMethodsNatural2023">#</a> Efficient Methods for Natural Language Processing: A Survey</div> <div class="author"> <a href="https://mtreviso.github.io/" rel="external nofollow noopener" target="_blank">Marcos Treviso</a>, Ji-Ung Lee, Tianchu Ji, Betty Aken, <em><strong>Qingqing Cao</strong></em>, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H. Martins, André F. T. Martins, Jessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell, <a href="https://www3.cs.stonybrook.edu/~niranjan/" rel="external nofollow noopener" target="_blank">Niranjan Balasubramanian</a>, Leon Derczynski, Iryna Gurevych, and Roy Schwartz</div> <div class="periodical"> <em>Transactions of the Association for Computational Linguistics</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2209.00099" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">trevisoEfficientMethodsNatural2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient {{Methods}} for {{Natural Language Processing}}: {{A Survey}}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Efficient {{Methods}} for {{Natural Language Processing}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and van Aken, Betty and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, Andr{\'e} F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{826--860}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2307-387X}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/tacl_a_00577}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-01-24}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://dl.acm.org/journal/imwut" rel="external nofollow noopener" target="_blank">IMWUT 2022</a></abbr></div> <div id="mobivqa" class="col-sm-8"> <div class="title"> <a href="#mobivqa">#</a> MobiVQA: Efficient On-Device Visual Question Answering</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="https://sites.google.com/view/prerna-khanna" rel="external nofollow noopener" target="_blank">Prerna Khanna</a>, <a href="http://niclane.org/" rel="external nofollow noopener" target="_blank">Nicholas D. Lane</a>, and <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a> </div> <div class="periodical"> <em>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://doi.org/10.1145/3534619" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/mobivqa.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mobivqa</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Khanna, Prerna and Lane, Nicholas D. and Balasubramanian, Aruna}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MobiVQA: Efficient On-Device Visual Question Answering}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{July 2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3534619}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3534619}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{44}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{visual question answering, on-device applications, mobile computing, edge computing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://2021.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2021</a></abbr></div> <div id="lal-etal-2021-irene" class="col-sm-8"> <div class="title"> <a href="#lal-etal-2021-irene">#</a> IrEne-viz: Visualizing Energy Consumption of Transformer Models</div> <div class="author"> <a href="https://ykl7.github.io/" rel="external nofollow noopener" target="_blank">Yash Kumar Lal</a>, Reetu Singh, <a href="http://harshtrivedi.me/" rel="external nofollow noopener" target="_blank">Harsh Trivedi</a>, <em><strong>Qingqing Cao</strong></em>, <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a>, and <a href="https://www3.cs.stonybrook.edu/~niranjan/" rel="external nofollow noopener" target="_blank">Niranjan Balasubramanian</a> </div> <div class="periodical"> <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, Nov 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://aclanthology.org/2021.emnlp-demo.29" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>IrEne is an energy prediction system that accurately predicts the interpretable inference energy consumption of a wide range of Transformer-based NLP models. We present the IrEne-viz tool, an online platform for visualizing and exploring energy consumption of various Transformer-based models easily. Additionally, we release a public API that can be used to access granular information about energy consumption of transformer models and their components. The live demo is available at \urlhttp://stonybrooknlp.github.io/irene/demo/.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lal-etal-2021-irene</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{I}r{E}ne-viz: Visualizing Energy Consumption of Transformer Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lal, Yash Kumar and Singh, Reetu and Trivedi, Harsh and Cao, Qingqing and Balasubramanian, Aruna and Balasubramanian, Niranjan}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Adel, Heike and Shi, Shuming}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online and Punta Cana, Dominican Republic}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.emnlp-demo.29}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.emnlp-demo.29}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{251--258}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://2021.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL 2021</a></abbr></div> <div id="cao-etal-2021-irene" class="col-sm-8"> <div class="title"> <a href="#cao-etal-2021-irene">#</a> IrEne: Interpretable Energy Prediction for Transformers</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="https://ykl7.github.io/" rel="external nofollow noopener" target="_blank">Yash Kumar Lal</a>, <a href="http://harshtrivedi.me/" rel="external nofollow noopener" target="_blank">Harsh Trivedi</a>, <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a>, and <a href="https://www3.cs.stonybrook.edu/~niranjan/" rel="external nofollow noopener" target="_blank">Niranjan Balasubramanian</a> </div> <div class="periodical"> <em>In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://aclanthology.org/2021.acl-long.167" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/StonyBrookNLP/IrEne" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models. IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives. IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage. IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. In contrast, existing energy models see an error of over 50%. We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. We release the code and data at \urlhttps://github.com/StonyBrookNLP/irene.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cao-etal-2021-irene</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{I}r{E}ne: Interpretable Energy Prediction for Transformers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Lal, Yash Kumar and Trivedi, Harsh and Balasubramanian, Aruna and Balasubramanian, Niranjan}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.acl-long.167}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.acl-long.167}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2145--2157}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://emdl21.github.io/" rel="external nofollow noopener" target="_blank">EMDL 2021</a></abbr></div> <div id="10.1145/3469116.3470011" class="col-sm-8"> <div class="title"> <a href="#10.1145/3469116.3470011">#</a> Are Mobile DNN Accelerators Accelerating DNNs</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, Alexandru E. Irimiea, Mohamed Abdelfattah, <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a>, and <a href="http://niclane.org/" rel="external nofollow noopener" target="_blank">Nicholas D. Lane</a> </div> <div class="periodical"> <em>In Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://doi.org/10.1145/3469116.3470011" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/ncs_emdl21.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/csarron/MobileAccelerator" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ncs_slides.pptx" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Deep neural networks (DNNs) are running on many mobile and embedded devices with the goal of energy efficiency and highest possible performance. However, DNN workloads are getting more computationally intensive, and simultaneously their deployment is ever-increasing. This has led to the creation of many purpose-built low-power neural accelerators to replace or augment traditional mobile CPUs and GPUs. In this work, we provide an in-depth study of one set of commercially-available mobile accelerators, the Intel Neural Compute Sticks (NCS). We perform a systematic measurement study of the latency and energy of this accelerator under a variety of DNNs including convolutional neural networks (CNNs) for vision tasks and attention-based Transformer models for NLP tasks. We compare to the mobile processors (CPU, GPU, and DSP) on a smartphone and a mobile board. Our study shows commercial mobile accelerators like NCS are not ready yet to provide the performance as claimed. We also point out directions in optimizing the model architectures to better suit these accelerators.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3469116.3470011</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Irimiea, Alexandru E. and Abdelfattah, Mohamed and Balasubramanian, Aruna and Lane, Nicholas D.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Are Mobile DNN Accelerators Accelerating DNNs}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450385978}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3469116.3470011}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3469116.3470011}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7-12}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Virtual, WI, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{EMDL'21}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://sites.google.com/view/sustainlp2020/home" rel="external nofollow noopener" target="_blank">SustaiNLP 2020</a></abbr></div> <div id="cao-etal-2020-towards" class="col-sm-8"> <div class="title"> <a href="#cao-etal-2020-towards">#</a> Towards Accurate and Reliable Energy Measurement of NLP Models</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a>, and <a href="https://www3.cs.stonybrook.edu/~niranjan/" rel="external nofollow noopener" target="_blank">Niranjan Balasubramanian</a> </div> <div class="periodical"> <em>In Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://aclanthology.org/2020.sustainlp-1.19" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/csarron/sustainlp2020-energy" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/sustainlp_slides.pptx" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at \urlhttps://github.com/csarron/sustainlp2020-energy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cao-etal-2020-towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Accurate and Reliable Energy Measurement of {NLP} Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Balasubramanian, Aruna and Balasubramanian, Niranjan}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Moosavi, Nafise Sadat and Fan, Angela and Shwartz, Vered and Glava{\v{s}}, Goran and Joty, Shafiq and Wang, Alex and Wolf, Thomas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.sustainlp-1.19}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.sustainlp-1.19}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{141--148}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://acl2020.org/" rel="external nofollow noopener" target="_blank">ACL 2020</a></abbr></div> <div id="cao-etal-2020-deformer" class="col-sm-8"> <div class="title"> <a href="#cao-etal-2020-deformer">#</a> DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="http://harshtrivedi.me/" rel="external nofollow noopener" target="_blank">Harsh Trivedi</a>, <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a>, and <a href="https://www3.cs.stonybrook.edu/~niranjan/" rel="external nofollow noopener" target="_blank">Niranjan Balasubramanian</a> </div> <div class="periodical"> <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://aclanthology.org/2020.acl-main.411" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/StonyBrookNLP/deformer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/deformer_slides.pptx" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Transformer-based QA models use input-wide self-attention – i.e. across both the question and the input passage – at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset. We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. We open source the code at \urlhttps://github.com/StonyBrookNLP/deformer.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cao-etal-2020-deformer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{D}e{F}ormer: Decomposing Pre-trained Transformers for Faster Question Answering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Trivedi, Harsh and Balasubramanian, Aruna and Balasubramanian, Niranjan}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.acl-main.411}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.acl-main.411}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4487--4497}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://www.sigmobile.org/mobisys/2019/" rel="external nofollow noopener" target="_blank">MobiSys 2019</a></abbr></div> <div id="deqa" class="col-sm-8"> <div class="title"> <a href="#deqa">#</a> DeQA: On-Device Question Answering</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, Noah Weber, <a href="https://www3.cs.stonybrook.edu/~niranjan/" rel="external nofollow noopener" target="_blank">Niranjan Balasubramanian</a>, and <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a> </div> <div class="periodical"> <em>In Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services</em>, Jul 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://doi.org/10.1145/3307334.3326071" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/deqa.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/deqa_slides.pptx" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Today there is no effective support for device-wide question answering on mobile devices. State-of-the-art QA models are deep learning behemoths designed for the cloud which run extremely slow and require more memory than available on phones. We present DeQA, a suite of latency- and memory- optimizations that adapts existing QA systems to run completely locally on mobile phones. Specifically, we design two latency optimizations that (1) stops processing documents if further processing cannot improve answer quality, and (2) identifies computation that does not depend on the question and moves it offline. These optimizations do not depend on the QA model internals and can be applied to several existing QA models. DeQA also implements a set of memory optimizations by (i) loading partial indexes in memory, (ii) working with smaller units of data, and (iii) replacing in-memory lookups with a key-value database. We use DeQA to port three state-of-the-art QA systems to the mobile device and evaluate over three datasets. The first is a large scale SQuAD dataset defined over Wikipedia collection. We also create two on-device QA datasets, one over a publicly available email data collection and the other using a cross-app data collection we obtain from two users. Our evaluations show that DeQA can run QA models with only a few hundred MBs of memory and provides at least 13x speedup on average on the mobile phone across all three datasets with less than a 1% drop in accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">deqa</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Weber, Noah and Balasubramanian, Niranjan and Balasubramanian, Aruna}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DeQA: On-Device Question Answering}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450366618}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3307334.3326071}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3307334.3326071}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{27-40}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{mobile devices, mobile systems, question answering}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Seoul, Republic of Korea}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{MobiSys '19}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://www.sigmobile.org/mobicom/2017/" rel="external nofollow noopener" target="_blank">MobiCom 2017</a></abbr></div> <div id="uiwear" class="col-sm-8"> <div class="title"> <a href="#uiwear">#</a> UIWear: Easily Adapting User Interfaces for Wearable Devices</div> <div class="author"> <a href="https://jianxux.github.io/" rel="external nofollow noopener" target="_blank">Jian Xu*</a>, <em><strong>Qingqing Cao*</strong></em>, Aditya Prakash, <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a>, and <a href="https://www.cs.unc.edu/~porter/" rel="external nofollow noopener" target="_blank">Donald E. Porter</a> </div> <div class="periodical"> <em>In Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking</em>, Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://doi.org/10.1145/3117811.3117819" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/uiwear.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/SBUNetSys/UIWear" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/uiwear_slides.pptx" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Wearable devices such as smartwatches offer exciting new opportunities for users to interact with their applications. However, the current wearable programming model requires the developer to write a custom companion app for each wearable form factor; the companion app extends the smartphone display onto the wearable, relays user interactions from the wearable to the phone, and updates the wearable display as needed. The development effort required to write a companion app is significant and will not scale to an increasing diversity of form factors. This paper argues for a different programming model for wearable devices. The developer writes an application for the smartphone, but only specifies a UI design for the wearable. Our UIWear system abstracts a logical model of the smartphone GUI, re-tailors the GUI for the wearable device based on the specified UI design, and compiles it into a companion app that we call the UICompanion app. We implemented UIWear on Android smartphones, AndroidWear smartwatches, and Sony SmartEyeGlasses. We evaluate 20 developer-written companion apps from the AndroidWear category on Google Play against the UIWear-created UICompanion apps. The lines-of-code required for the developer to specify the UI design in UIWear is an order-of-magnitude smaller compared to the companion app lines-of-code. Further, in most cases, the UICompanion app performed comparably or better than the corresponding companion app both in terms of qualitative metrics, including latency and energy, and quantitative metrics, including look-and-feel.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">uiwear</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu*, Jian and Cao*, Qingqing and Prakash, Aditya and Balasubramanian, Aruna and Porter, Donald E.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{UIWear: Easily Adapting User Interfaces for Wearable Devices}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450349161}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3117811.3117819}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3117811.3117819}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{369-382}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{accessibility, smartphone, wearable, android, smartwatch}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Snowbird, Utah, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{MobiCom '17}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://www.sigmobile.org/mobicom/2017/" rel="external nofollow noopener" target="_blank">MobiCom 2017</a></abbr></div> <div id="uiwear_demo" class="col-sm-8"> <div class="title"> <a href="#uiwear_demo">#</a> Demo: UIWear: Easily Adapting User Interfaces for Wearable Devices</div> <div class="author"> <a href="https://jianxux.github.io/" rel="external nofollow noopener" target="_blank">Jian Xu*</a>, <em><strong>Qingqing Cao*</strong></em>, <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a>, and <a href="https://www.cs.unc.edu/~porter/" rel="external nofollow noopener" target="_blank">Donald E. Porter</a> </div> <div class="periodical"> <em>In Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking</em>, Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://doi.org/10.1145/3117811.3124769" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/uiwear_demo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/uiwear_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://youtu.be/YEQ3HNeQnts" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Wearable devices such as smartwatches offer exciting new opportunities for users to interact with their applications. However, the current wearable programming model requires the developer to write a custom companion app for each wearable form factor; the companion app extends the smartphone display onto the wearable, relays user interactions from the wearable to the phone, and updates the wearable display as needed. The development effort required to write a companion app is significant and will not scale to an increasing diversity of form factors. This paper argues for a different programming model for wearable devices. The developer writes an application for the smartphone, but only specifies a UI design for the wearable. Our UIWear system abstracts a logical model of the smartphone GUI, re-tailors the GUI for the wearable device based on the specified UI design, and compiles it into a companion app that we call the UICompanion app. We implemented UIWear on Android smartphones, AndroidWear smartwatches, and Sony SmartEyeGlasses. We evaluate 20 developer-written companion apps from the AndroidWear category on Google Play against the UIWear-created UICompanion apps. The lines-of-code required for the developer to specify the UI design in UIWear is an order-of-magnitude smaller compared to the companion app lines-of-code. Further, in most cases, the UICompanion app performed comparably or better than the corresponding companion app both in terms of qualitative metrics, including latency and energy, and quantitative metrics, including look-and-feel.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">uiwear_demo</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu*, Jian and Cao*, Qingqing and Balasubramanian, Aruna and Porter, Donald E.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Demo: UIWear: Easily Adapting User Interfaces for Wearable Devices}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450349161}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3117811.3124769}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3117811.3124769}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{510-512}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{accessibility, smartphone, android wear, smartwatch, android}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Snowbird, Utah, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{MobiCom '17}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="http://www.cs.ucl.ac.uk/deepmobile_wkshp/index.html" rel="external nofollow noopener" target="_blank">EMDL 2017</a></abbr></div> <div id="mobirnn" class="col-sm-8"> <div class="title"> <a href="#mobirnn">#</a> MobiRNN: Efficient Recurrent Neural Network Execution on Mobile GPU</div> <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="https://www3.cs.stonybrook.edu/~niranjan/" rel="external nofollow noopener" target="_blank">Niranjan Balasubramanian</a>, and <a href="https://www3.cs.stonybrook.edu/~arunab/" rel="external nofollow noopener" target="_blank">Aruna Balasubramanian</a> </div> <div class="periodical"> <em>In Proceedings of the 1st International Workshop on Deep Learning for Mobile Systems and Applications</em>, Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://doi.org/10.1145/3089801.3089804" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/SBUNetSys/MobiRNN-EMDL17" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/mobirnn_slides.pptx" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In this paper, we explore optimizations to run Recurrent Neural Network (RNN) models locally on mobile devices. RNN models are widely used for Natural Language Processing, Machine translation, and other tasks. However, existing mobile applications that use RNN models do so on the cloud. To address privacy and efficiency concerns, we show how RNN models can be run locally on mobile devices. Existing work on porting deep learning models to mobile devices focus on Convolution Neural Networks (CNNs) and cannot be applied directly to RNN models. In response, we present MobiRNN, a mobile-specific optimization framework that implements GPU offloading specifically for mobile GPUs. Evaluations using an RNN model for activity recognition shows that MobiRNN does significantly decrease the latency of running RNN models on phones.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mobirnn</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Balasubramanian, Niranjan and Balasubramanian, Aruna}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MobiRNN: Efficient Recurrent Neural Network Execution on Mobile GPU}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450349628}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3089801.3089804}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3089801.3089804}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 1st International Workshop on Deep Learning for Mobile Systems and Applications}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-6}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{recurrent neural network, mobile GPU, renderscript, performance optimizations}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Niagara Falls, New York, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{EMDL '17}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container" align="center"> © Copyright 2024 Qingqing Cao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="external nofollow noopener">al-folio</a> theme. Last updated: October, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>